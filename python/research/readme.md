# ML (Personal) Research

This folder will hold my personal research efforts that will allow me to better understand the structure and features of neural nets. I may ocasionally implement research papers if they align with my research goals

Upcoming:
    1. Knowledge Distillation in Neural Nets
    2. Do Neural Nets learn better when pre-trained on random tasks vs random initilization?
    3. Does adding unrelated data to the dataset help the model perform better? Does adding data that are closely related to the task help the model perform better?
    4. How do Conv Nets learn? What are the layers learning?
    5. Can an MOE-like structure be used to allow a network to learn multiple tasks? Can the router learn to send specific tasks to a specific model?
    6. Inverse knowledge distillation - can we use a smaller model to "teach" a larger model to help speed up convergence?
    7. Compression of Neural Nets - can we reduce the size of a network without drastically affecting accuracy?
