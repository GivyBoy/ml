# ML (Personal) Research

This folder will hold my personal research efforts that will allow me to better understand the structure and features of neural nets. I may ocasionally implement research papers if they align with my research goals

Upcoming:<br>
    1. Knowledge Distillation in Neural Nets<br>
    2. Do Neural Nets learn better when pre-trained on random tasks vs random initilization?<br>
    3. Does adding unrelated data to the dataset help the model perform better? Does adding data that are closely related to the task help the model perform better?<br>
    4. How do Conv Nets learn? What are the layers learning?<br>
    5. Can an MOE-like structure be used to allow a network to learn multiple tasks? Can the router learn to send specific tasks to a specific model?<br>
    6. Inverse knowledge distillation - can we use a smaller model to "teach" a larger model to help speed up convergence?<br>
    7. Compression of Neural Nets - can we reduce the size of a network without drastically affecting accuracy?<br>
